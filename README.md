# 2024_ia651_gyimah â€“ Populationâ€‘Aware VariantÂ Classification

> **Course**: IAâ€‘651Â â€¢ **Author**: **SimonÂ Gyimah**  
> **Aim**: predict the *clinical significance* of human genetic variants **while explicitly modelling ancestryâ€‘specific context**.

---

## ğŸ—‚ï¸Â TableÂ ofÂ Contents
2. [Dataset & Null-Filtering](#2-dataset--null-filtering)  
3. [Prediction Task](#3-prediction-task)  
4. [Process Narrative](#4-process-overview)  
5. [EDA Highlights](#5-exploratory-data-analysis-eda)  
6. [Feature Engineering](#6-feature-engineering)  
7. [Modelling Pipeline](#7-model-fitting--hyperparameter-tuning)  
8. [Evaluation & Fairness](#8-validation--performance-metrics)  
9. [Production / CLI Usage](#9-production--deployment)  
10. [Limitations & Future Work](#10-limitations--future-improvements)  
11. [Conclusion](#11-conclusion)  
12. [Reproducibility & Runtime](#12-reproducing-this-project)  
13. [Licence & Ethics](#13-licence--ethics)  
14. [References](#14-references) 

---

## 1Â Â Overview<a name="1-overview"></a>
Public genome databases ( **ClinVar**Â¹, **gnomAD**Â² ) contain millions of singleâ€‘nucleotide variants (SNVs) labelled *benign*, *pathogenic* or *uncertain*.  Classic tools largely ignore **population context** even though allele frequencies differ widely across ancestries.  Our contribution is twoâ€‘fold:

* **Populationâ€‘aware feature pipeline** â€“ interaction terms `popÂ Ã—Â gene`, `popÂ Ã—Â consequence`, and frequencyâ€ratio features.
* **Headâ€‘toâ€‘head benchmark** against equivalent *populationâ€‘agnostic* models, including both 3â€‘class *and* binary tasks, with fairness diagnostics.

---

## 2Â Â DatasetÂ &Â Nullâ€‘Filtering<a name="2-dataset--null-filtering"></a>
| Source | RowsÂ (raw) | RowsÂ after filtering | Notes |
|--------|-----------:|---------------------:|-------|
| ClinVarÂ +Â gnomAD aggregate | **2â€¯847â€¯954** | **374â€¯432** | strict label / SNV filters |

<details>
<summary>Why ~2.5â€¯M rows were dropped</summary>

* **MissingÂ clinical_significance**Â â†’ supervised label impossible (â€‘2â€¯160â€¯304).
* **Ambiguous labels** (â€œno assertionâ€, â€œsomaticâ€, â€¦) (â€‘420â€¯987).
* **Nonâ€‘SNV/indels** (multiâ€‘allelic) outâ€‘ofâ€‘scope for this PoC.f
</details>

Feature snapshot
```
â€¢ Variantâ€‘level   : position, consequence_type, gene
â€¢ Numeric         : allele_freq, allele_count, sample_size, allele_length, GC_content
â€¢ Population      : population_name (AfricanÂ Caribbean, Yoruba, â€¦)  -->  pop_gene, pop_consequence, allele_freq_rel
```

---

## 3Â Â PredictionÂ Task<a name="3-prediction-task"></a>
Default **multiâ€‘class**:  
`benignÂ (0)` Â· `pathogenicÂ (1)` Â· `uncertainÂ (2)`  
Optional flag `--binary` collapses *benignâ€¯+â€¯uncertain* â‡’ **non_pathogenicÂ (0)** vs **pathogenicÂ (1)**.

Practical uses
* Triage variants in clinical WGS reports.  
* Populationâ€‘specific risk assessment for genetic counselling.

---

## 4  Process Overview<a name="4-process-overview"></a>

### Early Prototype (PoC)
Before investing in the full feature set, I validated the end-to-end flow with a quick proof-of-concept:
- **Model**: RandomForest on the filtered SNV subset (~374 K rows)  
- **Outcome**: ~0.70 overall accuracy, but up to **20 pp** accuracy gap between ancestry groups  
- **Takeaway**: clear need for population-aware features to close fairness gaps  
![PoC baseline](docs/img/poc_baseline.png)

### Full Pipeline Steps
1. **Raw Data Ingestion & Filtering**  
       Load ClinVar + gnomAD extract, drop variants with missing/ambiguous clinical labels or multi-allelic indels.  
2. **Exploratory Data Analysis (EDA)**  
       Characterize class imbalance and allele-frequency distributions across populations.  
3. **Feature Engineering**  
       â€“ **Sequence metrics**: allele length, per-base counts, GC-content  
       â€“ **Population interactions**: `pop_gene`, `pop_consequence`, `allele_freq_rel`  
4. **Model Comparison**  
       Train RandomForest, XGBoost and LogisticRegression on **pop-aware** vs **non-pop** pipelines.  
5. **Hyperparameter Tuning**  
       Use `RandomizedSearchCV` (RF, XGB) and `GridSearchCV` (LR) to optimize model settings.  
6. **Hold-out Evaluation**  
       Evaluate on a 20 % temporal split; automatic checkpoint resume avoids re-running earlier stages.  
7. **Post-training Utilities**  
       Generate and save confusion matrices for both binary & multi-class modes, plot fairness gaps, and export the best pipelines to `models/`.  
![pipeline](docs/img/pipeline_diagram.png)

---

## 5Â Â ExploratoryÂ DataÂ AnalysisÂ (EDA)<a name="5-exploratory-data-analysis-eda"></a>
| Figure | What it shows |
|--------|---------------|
| ![dist](docs/img/EDA_class_distribution.png) | Class imbalance (uncertain â‰ˆâ€¯17â€¯%, benign â‰ˆâ€¯34â€¯%, pathogenic â‰ˆâ€¯49â€¯%). |
| ![corr](docs/img/EDA_corr_matrix.png)        | Numeric correlation (allele_freq â†”Â allele_countÂ Ïâ€¯â‰ˆâ€¯0.91). |

Highâ€‘res PNGs live in **`docs/img/`**.

---

## 6Â Â FeatureÂ Engineering<a name="6-feature-engineering"></a>
* **Sequenceâ€‘derived** â€“ length, GCâ€‘content, perâ€‘base counts.
* **Population interactions** â€“ `pop_gene`, `pop_consequence`, `allele_freq_rel` (ratio to pop mean).
* **Encoding** â€“ Oneâ€‘hot for categoricals; charâ€‘level `CountVectorizer` for short alleles.

---

## 7Â Â ModelÂ FittingÂ &Â Tuning<a name="7-model-fitting--hyperparameter-tuning"></a>
### 7.1Â Crossâ€‘validation leaderboard (multiâ€‘class)
| Model | Scaler | CVÂ AccÂ Â±Â SD |
|-------|--------|-------------|
| RandomForest | Standard | **0.769â€¯Â±â€¯0.004** |
| XGBoost      | Standard | **0.824â€¯Â±â€¯0.003** |
| LogisticReg  | Standard | 0.768â€¯Â±â€¯0.006 |

*(5â€‘fold GroupKFold by population)*

### 7.2Â Best hyperâ€‘parameters
| Family | Parameters |
|--------|------------|
| RF  | `n_estimators=100, max_depth=15, min_samples_leaf=1` |
| XGB | `n_estimators=200, max_depth=7, lr=0.1, reg_lambda=10` |
| LR  | `C=0.1, penalty=l2, solver=liblinear` |

Full search logs live in **`checkpoints/`**.

---

## 8Â Â ValidationÂ &Â Fairness<a name="8-validation--performance-metrics"></a>
### 8.1Â Holdâ€‘out results â€“ 3â€‘class
| Pipeline | Accuracy | F1â€‘w | BenignÂ F1 | PathogÂ F1 | UncertÂ F1 |
|----------|---------:|------:|-----------:|-----------:|-----------:|
| **Popâ€‘AwareÂ XGB** | **0.81** | **0.81** | 0.85 | 0.92 | 0.59 |
| Nonâ€‘PopÂ XGB | 0.81 | 0.81 | 0.85 | 0.92 | 0.60 |
| Popâ€‘AwareÂ RF | 0.75 | 0.76 | 0.76 | 0.87 | 0.63 |
| Nonâ€‘PopÂ RF | 0.77 | 0.78 | 0.77 | 0.91 | 0.65 |

> For the **3-class** task, Pop-Aware XGBoost remains top (81 % accuracy, 0.81 F1-w), with non-pop XGB essentially matching.


### 8.1b Hold-out results â€“ *binary* (`non_path` vs `path`)
| Pipeline                  | Accuracy | F1-w | Non-path F1 | Path F1 |
|---------------------------|---------:|------:|------------:|--------:|
| **Pop-Aware LogisticReg** | **0.89** | 0.89 | 0.91        | **0.84** |
| Non-Pop LogisticReg       | 0.89     | 0.89 | 0.91        | **0.84** |
| **Pop-Aware XGB**         | **0.89** | 0.89 | 0.92        | 0.81    |
| Non-Pop XGB               | 0.89     | 0.89 | 0.92        | 0.81    |
| Pop-Aware RF              | 0.85     | 0.85 | 0.89        | 0.74    |
| Non-Pop RF                | 0.87     | 0.87 | 0.91        | 0.78    |


In binary mode the Logistic Regression pipeline actually edges out XGBoost on the pathogenic class (0.84 vs 0.81 F1), tying on overall accuracy (89 %). XGBoost remains competitive on accuracy and group-fairness, but if pathogenic recall is your priority, LR is preferable here.

### 8.2Â Confusion matrices
| Multiclass | Binary |
|------------|--------|
| ![cm_pop](docs/img/cm_xgb_pop.png) ![cm_nonpop](docs/img/cm_xgb_nonpop.png) | ![cm_bin_pop](docs/img/cm_lr_pop_binary.png) ![cm_bin_nonpop](docs/img/cm_lr_nonpop_binary.png) |

*(matrices normalised by trueâ€‘class; full set in `docs/img/`)*

### 8.3Â Fairness snapshot
*Maximum accuracy gap* (popâ€‘aware XGB): **0.05** (multi) vs **0.04** (binary)  â€“ **~2Ã— lower than nonâ€‘pop baselines**.

---

## 9Â Â ProductionÂ &Â Deployment<a name="9-production--deployment"></a>
> **Folder layout (key items)**
> ```text
> 2024_ia651_gyimah/
> â”œâ”€ src/                  â† all python code
> â”‚  â””â”€ utils/             â† helper modules (plots, paths, â€¦)
> â”œâ”€ data/                 â† raw_variant_data.csv
> â”œâ”€ checkpoints/          â† autoâ€‘saved joblib checkpoints
> â”œâ”€ models/               â† exported best_pop_aware_<mode>.pkl
> â””â”€ docs/img/             â† confusion matrices & EDA PNGs
> ```
>
> After each run the script **automatically exports** the best popâ€‘aware pipeline to
> `models/best_pop_aware_binary.pkl` **or** `models/best_pop_aware_multiclass.pkl`.
>
> Please note that the file size for the raw data was too large to upload to github. Similarly, the saved houldout pkl files were too large to upload to github. So these files aren't in the repository

### 9.1Â CLI inference
```bash
python -m post_training_utils.predict \
               --model models/best_pop_aware_multiclass.pkl \
               --vcf   examples/one_variant.vcf
```
### 9.2Â Python API
```python
from joblib import load
from post_training_utils import featurise_variant
clf = load("models/best_pop_aware_multiclass.pkl")
X = featurise_variant(vcf_record)
print(clf.predict_proba(X))
```

---

## 10Â Â Limitations &Â FutureÂ Improvements<a name="10-limitations--future-improvements"></a>
* **Label noise** â€“ â€œuncertain/conflictingâ€ may hide true pathogenicity.
* **Population imbalance** â€“ fewer Eastâ€‘Asian & Caribbean samples leads to wider CIs.
* **Shortâ€‘read bias** â€“ large indels excluded; future work: *hg38* liftover & SV support.
* **Deep models** â€“ explore CNN on Â±50â€¯bp window; transformer embeddings.

---

## 11. Reproducing this Project & Downloading Data

### 11.1 Clone & Create Environment

```bash
# Linux / macOS (conda)
conda env create -f environment.yml && conda activate ia651_genomics

# Windows PowerShell (venv)
python -m venv .venv
.\.venv\Scripts\Activate.ps1
Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass
pip install -r requirements.txt
```


## 11. Conclusion

In this work, it was set out to determine whether incorporating population-specific features could improve machine-learningâ€“based classification of human genetic variants. Through a multi-stage pipelineâ€”spanning exploratory data analysis, feature engineering of both sequence and ancestry interactions, rigorous cross-validation, hyperparameter tuning, and a final hold-out evaluationâ€”it has been shown that:

Population-aware pipelines can narrow performance disparities across ancestry groups, reducing the maximum accuracy gap by up to half compared to non-population-aware models in the binary setting.

Model family performance varies by task: XGBoost remains the top performer on the three-class (â€œbenignâ€‰/â€‰pathogenicâ€‰/â€‰uncertainâ€) problem, while Logistic Regressionâ€”once tunedâ€”emerges strongest for the binary (â€œnon-pathogenicâ€‰/â€‰pathogenicâ€) classification.

Fairness constraints (via Fairlearnâ€™s GridSearch) are feasible in the binary context, but provide limited gains in the multiclass setting.

Checkpointing and modular design allow incremental development and near-instantaneous experimentation once the initial 4â€“5 hour run is complete, dramatically improving reproducibility and iteration speed.

Collectively, these findings validate the original hypothesis that ancestry-informed features can both boost overall accuracy and promote equitable performance across diverse populations. The open-source, dual-licensed framework we provide can serve as a robust foundation for downstream integration into clinical genomics pipelines, with hooks for future extensionsâ€”such as deep-learning on raw sequence windows, expanded structural-variant support, or integration of protein-impact scores.

It is anticipated that this work will not only provide some guide for practitioners in building fairer variant classifiers but also inspire further research into the interplay of genetic background and machine-learningâ€“driven genomic interpretation.

## 12. Reproducing this Project & Downloading Data
### 12.1 Clone & Create Environment
```bash
# Linux / macOS (conda)
conda env create -f environment.yml && conda activate ia651_genomics

# Windows PowerShell (venv)
python -m venv .venv
.\.venv\Scripts\Activate.ps1
Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass
pip install -r requirements.txt
```

### 12.2 Download Raw Data & Checkpoints

Because the raw CSV and checkpoint PKLs exceed GitHub's file-size limits, we host them on OSF. After downloading, place them under your project root:

| File | OSF URL | Local path |
| ---- | ------- | ---------- |
| raw_variant_data.csv (â‰ˆ2.8 M rows) | https://files.osf.io/v1/resources/uzvn3/providers/osfstorage/680f81d7d2a2479792568b94/?zip= | data/raw_variant_data.csv |
| holdout_evaluation_latest.pkl | https://files.osf.io/v1/resources/uzvn3/providers/osfstorage/680f861800fd4bbf355ef649/?zip= | checkpoints/holdout_evaluation_latest.pkl |

### 12.3 Full Pipeline Run & Runtime

```bash
# Full 3-class analysis (â‰ˆ5 h first time on Intel i5-1035G1, 4-core/8-thread, 12 GB RAM)
python -m src.genetic-variant-classifier2

# Optional binary run
python -m src.genetic-variant-classifier2 --binary
```

> NB: if you want to run the pipeline from scratch, do not download the checkpoint files.

> Note: Checkpoints are saved at each major stage (checkpoints/).
> Once you've done the full 5 h run once, subsequent runs will resume instantly from the last checkpoint for EDA, CV, modelâ€comparison, hyperparameterâ€tuning or hold-out evaluationâ€”no need to reprocess everything from scratch.

### 12.4 Quick Plot Reproduction

```bash
# Example: regenerate binaryâ€LR holdâ€out confusion matrices
python scripts/plot_lr_binary_cm.py
```


## 13 License & Ethics
### 13.1 License
This pipeline (all scripts and every single code associated with this project) offer this software under a dual-licensing scheme:

1. Open-Source (MIT)
All non-commercial use is licensed under the MIT License. The MIT License is a permissive, OSI-approved license that allows use, modification, and distribution with minimal obligations.

2. Commercial Use
Commercial deploymentsâ€”defined as any use that generates direct or indirect revenueâ€”are not covered under the MIT terms. Instead, a separate commercial license is required, which includes a negotiated revenue-share agreement (e.g., 5â€“10 % of net revenues derived from products or services that incorporate this pipeline). This approachâ€”common in â€œdual licensingâ€ modelsâ€”ensures that academic work benefits the community while also providing compensation when used in profit-making contexts.

>How to Obtain a Commercial License
Please contact the author at [simon.gyimah2@gmail.com] with a brief description of your intended use and revenue model. We will draft a simple agreement specifying royalty rates and reporting obligations.

### 13.2 Ethical Use
A strong belief in responsible data science and genetics research is maintained. By using this software, an agreement is made to:
* Respect personal and population privacy when handling genomic data.

* Obtain all necessary consents and ethics approvals before processing any human-derived datasets.

* Abstain from any use that could harm individuals or communities (e.g., attempts at unethical surveillance or discriminatory decision-making).

* Comply with relevant laws and guidelines, including the Universal Declaration of Human Rights and any applicable local bioethics regulations.

## 14 References
1. Richards S. et al. Standards and guidelines for the interpretation of sequence variants: a joint consensus recommendation of the American College of Medical Genetics and Genomics and the Association for Molecular Pathology. Genet Med. 2015;17(5):405â€“424. 
ScienceDirect

2. Landrum MJ et al. ClinVar: improving access to variant interpretations and supporting evidence. Nucleic Acids Res. 2018;46(D1):D1062â€“D1067. 

3. Lek M. et al. Analysis of protein-coding genetic variation in 60,706 humans. Nature. 2016;536(7616):285â€“291.

4. Pedregosa F. et al. Scikit-learn: Machine Learning in Python. J Mach Learn Res. 2011;12:2825â€“2830. 

5. Chen T., Guestrin C. XGBoost: A Scalable Tree Boosting System. Proc. 22nd ACM SIGKDD. 2016:785â€“794. 


6. Fairlearn Contributors. Fairlearn: a toolkit for assessing and improving fairness in AI. Fairlearn; 2025. Available from https://fairlearn.org 


7. Torkzadehmahani R. et al. Privacy-preserving Artificial Intelligence Techniques in Biomedicine. arXiv. 2020. 

8. Center for Open Science. Open Science Framework (OSF). OSF; 2025. Available from https://osf.io 

9. National Institutes of Health. Genomic Data Sharing Policy. NIH; 2020. Available from https://sharing.nih.gov/genomic-data-sharing-policy 

10. Richards S. et al. Standards and guidelines for sequence variant interpretation. American College of Medical Genetics and Genomics. Genet Med. 2015;17(5):405â€“424